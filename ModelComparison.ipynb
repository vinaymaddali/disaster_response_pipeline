{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "### 1. Import libraries and load data from database. Template as provided by Udacity.\n",
    "- Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vinaymaddali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vinaymaddali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/vinaymaddali/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26180, 36)\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///response.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "X = df['message']\n",
    "category_cols = []\n",
    "for col in df.columns:\n",
    "    if col not in ['id', 'message', 'original', 'genre']:\n",
    "        category_cols.append(col)\n",
    "\n",
    "Y = df[category_cols].values\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function to tokenize input text data. Called in pipeline sentence by sentence.\n",
    "    Input: text: sentence as string.\n",
    "    return: clean_tokens: list of clean tokens in sentence.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ML pipeline\n",
    "CountVectorizer, tokenize and TfidfTransformer. Used RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8640, 36)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Prints macro averaged f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(category_cols, y_test, preds):\n",
    "    \"\"\"\n",
    "    Function to calculate f1-score, precision and recall of each category.\n",
    "    Uses classification report function of scikit learn and reports the macro average which is the non-weighted\n",
    "    average of each category.\n",
    "    Input: category_cols: category columns in data as list\n",
    "           y_test: Labels as numpy array\n",
    "           preds: Output predictions from classifier\n",
    "    Output: dict: results dictionary from classification report function.\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    avg_calc = {'f1_score': [], 'precision': [], 'recall': []}\n",
    "    print(\"Category : f1-score, precision, recall\\n\")\n",
    "    for ix, col in enumerate(category_cols):\n",
    "        results[col] = classification_report(y_test[:,ix], preds[:, ix], output_dict=True)\n",
    "        f1_score, precision, recall = results[col]['macro avg']['f1-score'], results[col]['macro avg']['precision'], \\\n",
    "                                      results[col]['macro avg']['recall']\n",
    "        avg_calc['f1_score'].append(f1_score)\n",
    "        avg_calc['precision'].append(precision)\n",
    "        avg_calc['recall'].append(recall)\n",
    "        print(\"{} : {}, {}, {}\".format(col, f1_score, precision, recall))\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"Avg. across categories:\")\n",
    "    print(\"f1-score: {}\".format(np.mean(avg_calc['f1_score'])))\n",
    "    print(\"precision: {}\".format(np.mean(avg_calc['precision'])))\n",
    "    print(\"recall: {}\".format(np.mean(avg_calc['recall'])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : f1-score, precision, recall\n",
      "\n",
      "related : 0.471014583506018, 0.7993475368313055, 0.43857022141339486\n",
      "request : 0.7530020515096362, 0.898842806108806, 0.7019529367646014\n",
      "offer : 0.4988108358953536, 0.4976273148148148, 0.5\n",
      "aid_related : 0.7545212596325008, 0.7785532216062954, 0.7478049955725492\n",
      "medical_help : 0.5208541985680587, 0.8188781444189679, 0.5210477984184175\n",
      "medical_products : 0.5338464016085032, 0.8476721235341924, 0.5239055751734342\n",
      "search_and_rescue : 0.5088013674353031, 0.8193768820940468, 0.5080441467831921\n",
      "security : 0.49503214494447695, 0.49016203703703703, 0.5\n",
      "military : 0.5086469769822447, 0.7962233549582947, 0.5085617040369255\n",
      "child_alone : 1.0, 1.0, 1.0\n",
      "water : 0.6339220130440816, 0.9224057014493483, 0.5877185261303381\n",
      "food : 0.7544277291656918, 0.8970943543287032, 0.6968498759341242\n",
      "shelter : 0.6446019706082937, 0.9088085377508761, 0.5991878109676504\n",
      "clothing : 0.5330899508660207, 0.6854686984511953, 0.5198555543146133\n",
      "money : 0.5239643330945809, 0.8640523632993513, 0.51526620212233\n",
      "missing_people : 0.49746990054091783, 0.49496527777777777, 0.5\n",
      "refugees : 0.5024895427828852, 0.8812970469021424, 0.5060560611929806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinaymaddali/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death : 0.5790585562072036, 0.9129144851657941, 0.5496363636363637\n",
      "other_aid : 0.4807419825672813, 0.7763900174114916, 0.50710559685044\n",
      "infrastructure_related : 0.48391021976666004, 0.7157328085204908, 0.5007810353632779\n",
      "transport : 0.5751045765369109, 0.8834825971659267, 0.5475554804983458\n",
      "buildings : 0.5405719938631587, 0.9242160278745645, 0.5284787993759225\n",
      "electricity : 0.5239419925253087, 0.9072272411396803, 0.5148219344394982\n",
      "tools : 0.4986363372599083, 0.4972800925925926, 0.5\n",
      "hospitals : 0.4971774428213932, 0.49438657407407405, 0.5\n",
      "shops : 0.4983743613562471, 0.49675925925925923, 0.5\n",
      "aid_centers : 0.4966501602097291, 0.4933449074074074, 0.5\n",
      "other_infrastructure : 0.488484992007578, 0.47748842592592594, 0.5\n",
      "weather_related : 0.8195838509185903, 0.8727951671177906, 0.7914176504414301\n",
      "floods : 0.7535971697109529, 0.9399954933001691, 0.6863053319919517\n",
      "storm : 0.7383125005260178, 0.8808574879227054, 0.681616289953832\n",
      "fire : 0.5075449182271377, 0.74455892567724, 0.5052046441440055\n",
      "earthquake : 0.8858571886619855, 0.9288746210094525, 0.8521961752045092\n",
      "cold : 0.5420586832287633, 0.9899779863283513, 0.5247252747252747\n",
      "other_weather : 0.5266421175211659, 0.9255800464037123, 0.5203792026029384\n",
      "direct_report : 0.6974850605336138, 0.868446728611461, 0.6577448475395262\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avg. across categories:\n",
      "f1-score: 0.5907841490176159\n",
      "precision: 0.7814190081742012\n",
      "recall: 0.5761886120997741\n"
     ]
    }
   ],
   "source": [
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "results = get_results(category_cols, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Can improve using Grid search of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [100, 250],\n",
    "    'clf__estimator__max_depth': [None, 2, 4]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1)\n",
    "rf_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : f1-score, precision, recall\n",
      "\n",
      "related : 0.4639718473548342, 0.7966512506678711, 0.4346759718234905\n",
      "request : 0.7539916695591808, 0.896097169519821, 0.7033097767403969\n",
      "offer : 0.4988108358953536, 0.4976273148148148, 0.5\n",
      "aid_related : 0.7620068372909544, 0.7836365236202745, 0.7553016917959576\n",
      "medical_help : 0.520591681836294, 0.7950552646887725, 0.5208591666276731\n",
      "medical_products : 0.546593457294148, 0.8490578567278161, 0.5310833237978911\n",
      "search_and_rescue : 0.5009015288110426, 0.8192659488248234, 0.5040220733915961\n",
      "security : 0.4950026301946344, 0.4901608982521125, 0.4999409681227863\n",
      "military : 0.5284805653710247, 0.8769169620085455, 0.519051214526436\n",
      "child_alone : 1.0, 1.0, 1.0\n",
      "water : 0.6239478264516535, 0.9096874870819727, 0.5811493514070077\n",
      "food : 0.7513985298850314, 0.8950175187012945, 0.6941225424854579\n",
      "shelter : 0.6779423298089223, 0.8954502714607555, 0.6252656155985071\n",
      "clothing : 0.5401498174863141, 0.7075038918883111, 0.5239205949650197\n",
      "money : 0.5142502837996911, 0.8222724113968033, 0.5101379969941249\n",
      "missing_people : 0.49746990054091783, 0.49496527777777777, 0.5\n",
      "refugees : 0.49649257227922644, 0.9811877749479045, 0.5030581039755352\n",
      "death : 0.5679012345679012, 0.884059415810986, 0.5431048951048951\n",
      "other_aid : 0.4744842324447245, 0.730160787497186, 0.504027370289957\n",
      "infrastructure_related : 0.48394407941333506, 0.9657367750897095, 0.5008431703204047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinaymaddali/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transport : 0.5582138444478777, 0.8735208458253283, 0.5374895410336913\n",
      "buildings : 0.5267382145822442, 0.8738247243180499, 0.5209253874949027\n",
      "electricity : 0.5182988000382496, 0.8905037637521714, 0.5118457439633077\n",
      "tools : 0.4986363372599083, 0.4972800925925926, 0.5\n",
      "hospitals : 0.4971774428213932, 0.49438657407407405, 0.5\n",
      "shops : 0.4983743613562471, 0.49675925925925923, 0.5\n",
      "aid_centers : 0.4966501602097291, 0.4933449074074074, 0.5\n",
      "other_infrastructure : 0.488484992007578, 0.47748842592592594, 0.5\n",
      "weather_related : 0.8226000849471748, 0.8733420554078859, 0.7950627473531103\n",
      "floods : 0.7437124253708057, 0.9360148041467413, 0.677521524495812\n",
      "storm : 0.7714206492944775, 0.8912245613942994, 0.7146109718149678\n",
      "fire : 0.4972067039106145, 0.49450167843500403, 0.4999414862492686\n",
      "earthquake : 0.8953223583583294, 0.9302476905029571, 0.8667111692907158\n",
      "cold : 0.5470505617977528, 0.9900347624565469, 0.5274725274725275\n",
      "other_weather : 0.5115599580135088, 0.8982710809533575, 0.5124065374548745\n",
      "direct_report : 0.7084333984124496, 0.8710793806835868, 0.6670077552778975\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avg. across categories:\n",
      "f1-score: 0.5910614486975979\n",
      "precision: 0.7797870946642427\n",
      "recall: 0.5773574783296727\n"
     ]
    }
   ],
   "source": [
    "preds = rf_cv.predict(X_test)\n",
    "results_rf_cv = get_results(category_cols, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not a big improvement. Try other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Adaboost - shows some improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : f1-score, precision, recall\n",
      "\n",
      "related : 0.5265875309990461, 0.6586718431888509, 0.48417719526693653\n",
      "request : 0.7907862460445884, 0.8557361402699644, 0.7534624616481147\n",
      "offer : 0.5194986072423398, 0.5691118796644107, 0.5118462441747339\n",
      "aid_related : 0.7513618879001609, 0.7713954171748223, 0.7452081333591691\n",
      "medical_help : 0.6251327524069452, 0.7629357306454853, 0.590208401572224\n",
      "medical_products : 0.7016789267406992, 0.8209652002295817, 0.6507948123646743\n",
      "search_and_rescue : 0.602004716981132, 0.7210741389375365, 0.5690462009699652\n",
      "security : 0.5115789473684211, 0.640324449594438, 0.5084103062712688\n",
      "military : 0.6719393743144606, 0.7738609052471608, 0.6277671933046604\n",
      "child_alone : 1.0, 1.0, 1.0\n",
      "water : 0.8263652915496049, 0.8480452200478428, 0.8076250190200722\n",
      "food : 0.8634927122821303, 0.8864838154559547, 0.8438416984973114\n",
      "shelter : 0.7930081402921406, 0.8579437595956534, 0.7513163339350288\n",
      "clothing : 0.7249067633455244, 0.8096766127687948, 0.6773354295712736\n",
      "money : 0.6822033898305084, 0.782074072396653, 0.6361524798469737\n",
      "missing_people : 0.598384090094297, 0.7455906242747737, 0.5625753415821027\n",
      "refugees : 0.6235069740610024, 0.7788796156570478, 0.5832811877494849\n",
      "death : 0.7495230485497658, 0.8506003890938373, 0.6964848484848485\n",
      "other_aid : 0.5564640857797616, 0.7148412957332558, 0.5477148393366987\n",
      "infrastructure_related : 0.5282109709249385, 0.7214322537573157, 0.5234931958554622\n",
      "transport : 0.6364242776566448, 0.8389862621084976, 0.590646421690169\n",
      "buildings : 0.7137349397590362, 0.8471697352956367, 0.658992790049067\n",
      "electricity : 0.6638305909917634, 0.7518985862834443, 0.6226983002832861\n",
      "tools : 0.4985490423679629, 0.4972791478522635, 0.49982543931106715\n",
      "hospitals : 0.5214423772985672, 0.5570450324976788, 0.5142348410889244\n",
      "shops : 0.4982287008537081, 0.49675738274464387, 0.49970876048462254\n",
      "aid_centers : 0.534962996187486, 0.6602898550724637, 0.521152620170853\n",
      "other_infrastructure : 0.5225712348950181, 0.6449098312972659, 0.5174622441963099\n",
      "weather_related : 0.8363093708244247, 0.8753813633583583, 0.8124114602712851\n",
      "floods : 0.8060562302826056, 0.9251262998227656, 0.7445150554489729\n",
      "storm : 0.8090620313388659, 0.8759894047143861, 0.7658219950467703\n",
      "fire : 0.6946594788109126, 0.8174741634267105, 0.6412275568969235\n",
      "earthquake : 0.899126157856839, 0.9265518770322199, 0.8757292973293869\n",
      "cold : 0.6953276520717014, 0.8117394199915239, 0.643830926699217\n",
      "other_weather : 0.5992921537235929, 0.7498209810239885, 0.5674446890886966\n",
      "direct_report : 0.7519988972860274, 0.8274370179590772, 0.7165021896934429\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avg. across categories:\n",
      "f1-score: 0.675783627469795\n",
      "precision: 0.7687083256726196\n",
      "recall: 0.6461929419599999\n"
     ]
    }
   ],
   "source": [
    "# Adaboost\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [50,100,200],\n",
    "    'clf__estimator__learning_rate': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "adb_cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1)\n",
    "adb_cv.fit(X_train, y_train);\n",
    "\n",
    "preds = adb_cv.predict(X_test)\n",
    "results_adb_cv = get_results(category_cols, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 XGBoost - does even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category : f1-score, precision, recall\n",
      "\n",
      "related : 0.6274757975918573, 0.7848652172641625, 0.5667525792298598\n",
      "request : 0.8016831873472638, 0.8520292908553305, 0.7693782801392477\n",
      "offer : 0.4987817612252001, 0.49762704016668596, 0.4999418537039191\n",
      "aid_related : 0.7561275413624406, 0.76808130666566, 0.7511659728694315\n",
      "medical_help : 0.6715746538621982, 0.7742938626755876, 0.6310742361143605\n",
      "medical_products : 0.7035157911769848, 0.8001779359430605, 0.6571219108493817\n",
      "search_and_rescue : 0.6147405660377359, 0.747974314068885, 0.5774477033219482\n",
      "security : 0.5167400665633026, 0.6442251963870139, 0.5112334189874297\n",
      "military : 0.7000115782383898, 0.7995964844852688, 0.6523624209164071\n",
      "child_alone : 1.0, 1.0, 1.0\n",
      "water : 0.8422657262653125, 0.8685843683869512, 0.8199715193608983\n",
      "food : 0.8769596278083266, 0.8919572964140037, 0.8634136556681365\n",
      "shelter : 0.8092521430682825, 0.848771375030255, 0.7794056207558975\n",
      "clothing : 0.7445476897531692, 0.8223467600700525, 0.697660632823306\n",
      "money : 0.679161253013496, 0.7904909348202083, 0.6312611012433392\n",
      "missing_people : 0.6125103164076373, 0.7457046668214534, 0.5739526764152122\n",
      "refugees : 0.6540468956502717, 0.7980874665306643, 0.6075054325214073\n",
      "death : 0.785171709302181, 0.8649212767920293, 0.73604662004662\n",
      "other_aid : 0.6086136556974309, 0.7243031982947838, 0.5844610793774935\n",
      "infrastructure_related : 0.5608285804142903, 0.7078132474119803, 0.5434273055579247\n",
      "transport : 0.6658288079691478, 0.8122057438794728, 0.6172816666794605\n",
      "buildings : 0.7345235559019916, 0.8478718552405489, 0.6807961276629919\n",
      "electricity : 0.6671849677040516, 0.7548920137483683, 0.6256744907594766\n",
      "tools : 0.4986363372599083, 0.4972800925925926, 0.5\n",
      "hospitals : 0.5619223896383045, 0.6486801450233073, 0.5401836193133343\n",
      "shops : 0.498345236021599, 0.49675888413010766, 0.4999417520969245\n",
      "aid_centers : 0.5654076373269045, 0.7751043599257885, 0.538719877597858\n",
      "other_infrastructure : 0.5354957660028281, 0.6730142091777312, 0.5249925303125991\n",
      "weather_related : 0.8446148349734603, 0.8705596971227041, 0.8264448804248554\n",
      "floods : 0.8235907811956624, 0.9106430155210643, 0.7708109119835291\n",
      "storm : 0.8341185706979131, 0.8646503405850048, 0.8093772851815371\n",
      "fire : 0.6500185397942329, 0.773419339841934, 0.604326937883034\n",
      "earthquake : 0.9003035941211481, 0.9238028235763229, 0.879823224120726\n",
      "cold : 0.7279227258112099, 0.8167362517740757, 0.679190518632467\n",
      "other_weather : 0.6166942771639847, 0.7286707518265378, 0.583004767183512\n",
      "direct_report : 0.7654528423492664, 0.8267219410645705, 0.7327094661292873\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avg. across categories:\n",
      "f1-score: 0.6931685945754829\n",
      "precision: 0.7764684084476158\n",
      "recall: 0.662968390996217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinaymaddali/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [100, 250],\n",
    "    'clf__estimator__max_depth': [None, 2, 4]\n",
    "}\n",
    "\n",
    "xgb_cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1)\n",
    "xgb_cv.fit(X_train, y_train);\n",
    "\n",
    "preds = xgb_cv.predict(X_test)\n",
    "results_xgb_cv = get_results(category_cols, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 word2vec feature - potential for improvement, might need more external data to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinaymaddali/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def test_tokenize(text):\n",
    "    \"\"\"\n",
    "    New tokenize function to support doc2vec function.\n",
    "    Reference: https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
    "    \"\"\"\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "\n",
    "corpus = [list(test_tokenize(doc)) for doc in X_train]\n",
    "corpus = [\n",
    "    TaggedDocument(words, ['d{}'.format(idx)])\n",
    "    for idx, words in enumerate(corpus)\n",
    "]\n",
    "\n",
    "model = Doc2Vec(corpus, size=100, min_count=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, mdl):\n",
    "    \"\"\"\n",
    "    Function to infer wordvec from input\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    return mdl.infer_vector(x)\n",
    "\n",
    "class gensim_word2vec():\n",
    "    \"\"\"\n",
    "    word2vec class to be input to pipeline. Has a fit and transform as required by scikit-learn pipeline.\n",
    "    Reference: https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Tokenizes data and other preprocessing before the Doc2Vec class is used to train the model.\n",
    "        \"\"\"\n",
    "        # Preprocessing \n",
    "        corpus = [list(test_tokenize(doc)) for doc in X]\n",
    "        corpus = [\n",
    "            TaggedDocument(words, ['d{}'.format(idx)])\n",
    "            for idx, words in enumerate(corpus)\n",
    "        ]\n",
    "        self.model = Doc2Vec(corpus, size=100, min_count=0)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Uses the trained model in fit to get the vector for token.\n",
    "        \"\"\"\n",
    "        \n",
    "        corpus = [list(test_tokenize(doc)) for doc in X]\n",
    "        \n",
    "        count_miss = 0\n",
    "        out = []\n",
    "        for doc in corpus:\n",
    "            try:\n",
    "                out.append(self.model.infer_vector(doc))\n",
    "            except:\n",
    "                print(w)\n",
    "                count_miss+=1\n",
    "                out.append(np.zeros((100)))\n",
    "         \n",
    "        print(count_miss)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('word2vec', gensim_word2vec()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators': [100, 250],\n",
    "    'clf__estimator__max_depth': [None, 2, 4]\n",
    "}\n",
    "\n",
    "wv_cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1)\n",
    "wv_cv.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Category : f1-score, precision, recall\n",
      "\n",
      "related : 0.4608824725321503, 0.7858028604368527, 0.4268743616317694\n",
      "request : 0.6978706560475011, 0.780802118006706, 0.6646345127603811\n",
      "offer : 0.4988108358953536, 0.4976273148148148, 0.5\n",
      "aid_related : 0.6615039226560255, 0.6771376510948488, 0.65898408890567\n",
      "medical_help : 0.5099974269912964, 0.7848354773003787, 0.5152338449300454\n",
      "medical_products : 0.5354598562787571, 0.7018498238545796, 0.5251234507124084\n",
      "search_and_rescue : 0.4967187316500294, 0.585871453387377, 0.50180257928067\n",
      "security : 0.49503214494447695, 0.49016203703703703, 0.5\n",
      "military : 0.4949727019950884, 0.6084993052339046, 0.5015686970439185\n",
      "child_alone : 1.0, 1.0, 1.0\n",
      "water : 0.5318638475338431, 0.7158128225741143, 0.5248509611021611\n",
      "food : 0.5706472353763159, 0.7148419713332709, 0.5545479135733171\n",
      "shelter : 0.5459651701349313, 0.6955083032873142, 0.5363031306159084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinaymaddali/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing : 0.5188531192681067, 0.6430475086906141, 0.5117841791309776\n",
      "money : 0.5313786548014091, 0.6891531322505801, 0.5198023409391083\n",
      "missing_people : 0.49746990054091783, 0.49496527777777777, 0.5\n",
      "refugees : 0.508130419202584, 0.7541272031943025, 0.5088735781361569\n",
      "death : 0.6143890500967757, 0.7951774588467558, 0.5760233100233101\n",
      "other_aid : 0.4915656016615112, 0.6296064238254877, 0.5109877576394343\n",
      "infrastructure_related : 0.49852356949598364, 0.7161832946635731, 0.5078103536327784\n",
      "transport : 0.5029704627227847, 0.7082237340728126, 0.5071704370120749\n",
      "buildings : 0.5685035360024076, 0.7395227324955551, 0.5463227846768856\n",
      "electricity : 0.5219085713382292, 0.6376073565225696, 0.514172737083502\n",
      "tools : 0.4986072423398329, 0.4972797777520546, 0.49994181310368907\n",
      "hospitals : 0.49714817832615527, 0.4943859242967936, 0.49994147255062626\n",
      "shops : 0.4983161073046104, 0.4967585089141005, 0.499883504193849\n",
      "aid_centers : 0.4966501602097291, 0.4933449074074074, 0.5\n",
      "other_infrastructure : 0.500763883365133, 0.6563131396773874, 0.5058813467807438\n",
      "weather_related : 0.7062050300044285, 0.7488226230394842, 0.6880495890132936\n",
      "floods : 0.5762487419884528, 0.736068351883044, 0.5545906251462263\n",
      "storm : 0.62860932171277, 0.7849364190914745, 0.5930117578814217\n",
      "fire : 0.49706036439839335, 0.49449849432476256, 0.49964891749561147\n",
      "earthquake : 0.7272317026290719, 0.8207202696992523, 0.6818167615846232\n",
      "cold : 0.5108640269930592, 0.8646364057433997, 0.5081826426115856\n",
      "other_weather : 0.4958078601117701, 0.6747972190034763, 0.5041899996083593\n",
      "direct_report : 0.6670374457627185, 0.7500887207289841, 0.6405214633587661\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avg. across categories:\n",
      "f1-score: 0.5570546653420168\n",
      "precision: 0.6766393339517458\n",
      "recall: 0.5496814142266466\n"
     ]
    }
   ],
   "source": [
    "preds = wv_cv.predict(X_test)\n",
    "results_wv_cv = get_results(category_cols, y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(xgb_cv, open('xgb_classifier.pkl', 'wb'))\n",
    "pkl.dump(wv_cv, open('wv_classifier.pkl', 'wb'))\n",
    "pkl.dump(adb_cv, open('adb_classifier.pkl', 'wb'))\n",
    "pkl.dump(rf_cv, open('rf_classifier.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
